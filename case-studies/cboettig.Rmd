##### Introduction
*Please answer these introductory questions for your case study in a few sentences.*

1) Who are you and what is your research field? Include your name, affiliation, discipline, and the background or context of your overall research that is necessary specifically to introduce your specific case study.

[Carl Boettiger](http://carlboettiger.info), [UC Berkeley ESPM](http://ourenvironment.ucberkeley.edu)

I'm a theoretical ecologist working on problems of forecasting and decision-making in ecological systems. My work involves developing new computational and frequently data-intesive approaches to these problems. 

2) Define what the term "reproducibility" means to you generally and/or in the particular context of your case study.

Reproducibility in this context is 'computational reproducibility.'  It means a good-faith effort to make sure that the analysis can produce qualitatively identical results while running on comparable hardware.  This means certain things do not need to be reproduced: e.g. how long the code takes to run may vary by hardware and operating system, but this is okay.  Nor am I not concerned with bitwise identical results.  More controversially, I am not concerned with reproducing stochastic random draws -- rather, I expect reproducible results to be robust to the details of stochastic seed or choice of random number generator.

I am also concerned that reproducibility is modular -- that individual components of the analysis can be reproduced (and thus recombined or otherwise modified), and not merely provide a black box that can only replicate final outputs without variation or adjustment.  

Lastly, I should comment on _who_ can reproduce the analysis.  Like the paper itself, the analysis requires a certain degree of expertise to understand, and I do not expect that individuals with no familiarity with programming, statistics, or scientific process can reproduce the analysis.  However, I do expect that researchers with some scientific background in my area (e.g. the broadest readership of the journal in which it is published) and with minimal familiarity with the R language or similar computing langauage can reproduce the overall results after suitable investment of time and effort in reading the documentation.

##### Workflow diagram

```{r echo=FALSE}
library("DiagrammeR") # version >= 0.7

n <- c("Open Lab Notebook", "Github public\n repository",)

nodes <- create_nodes(nodes = nodes, 
                      shape = "circle", 
                      color = c(rep("PowderBlue", 4), rep("Linen", 4)),
                      style = "filled")
edges <- create_edges(edge_from = n, 
                      edge_to = c(n[-1], n[1]),
                      color = "gray", penwidth = 4)

graph <- create_graph(nodes = nodes, edges = edges, 
                      graph_attrs = c("layout = circo"))

render_graph(graph)
```

```{r echo=FALSE}

## Generate a png or pdf version
basename <- "figure1"
width <- 300
height <- 300
out <- render_graph(graph, output = "SVG")
writeLines(out, paste0(basename, ".svg"))
system(paste0("inkscape --export-png ", basename, ".png", 
              "-w ", width, " -h ", height, 
#              " --export-dpi 300 ", 
              basename, ".svg")) 

system(paste0("inkscape --export-pdf ", basename, ".pdf ", basename, ".svg")) 


```


- Explore new ideas in open lab notebook

Open Lab notebook is hosted on Github (using gh-pages), uses `.Rmd` files for entries that contain code and narrative.  The computational environment needed to execute the notebook (primarily R + some packages, along with Ruby's Jekyll for website rendering) is provided by a Docker container, which is automatically built by Docker Hub using a Dockerfile in the repository's directory.  As new explorations introduce new dependencies, these are added to the Dockerfile. This allows me to build the notebook on my laptop or on a cloud machine, including executing all computational R code.  Because the `.Rmd` engine `knitr` caches results, I can avoid long build times.  The cache is preserved on a seprate Docker data container that can be linked to the execution container (or ommitted to regenerate results from scratch). The source files and generated HTML are thus both under version control on Git.  Any commits to the site cause the continuous integration service, circleCI, to rebuild the site (using the Docker containers for runtime and cached environment).
Each year I archive the github repository that contains that year's notebook on figshare, adding the DOI badge to the repository's README.

At this time a given exploration might not have a particular project connected to it -- it might build from several existing projects, a paper I'm reading, or represent an entirely new exploration.  I use categories and tags in the notebook to associate the post with relevant projects or themes, which makes it easier to come back to.  (Figuring out appropriate tags is harder than it sounds!)

Though each entry is dated, I will often continue to modify an entry long after the date is was first created.  The notebook links to the full history of each page on Github.  


- Create public GitHub repository for project, using an R package layout

Eventually multiple entries will relate to the same project.  At this point, I frequently want to reuse code first developed in a previous entry.  This is my signal that it is time to create a new project on Github.  (Figuring this out is much harder than it sounds!)  I create a new public Github repo using a name that matches a tag in the relevant notebook entries.  In the `R/` directory I store functions that provide these reusable bits.  For non-trivial functions, I try and develop unit tests (in the `/tests` directory) -- these usually come directly from the interactive tests I write in the notebook when first creating these functions. I also add minimal Roxygen documentation to the functions I create, usually just to remind me what the input and outputs are. Data goes in the `/data` directory; or more frequently, as R scripts that either simulate or download and clean the data from external sources.

Notebook pages do not load these functions as a single package -- as the package is constantly changing this is unlikely to continue to work anyway.  Instead, they source in the script directly from the version-explicit links on Github.  (I learned this the hard way). This avoids the burden of making sure the 'package' is always installable, it just serves as a convenient organizational skeleton.  

I continue to develop, test, and explore results in the pages of the notebook, adding and modifying functions as necessary.  This ususally involves plenty of mistakes and dead ends that are captured in the history of an individual page (when I modify an existing workflow to correct the results) or are left as dead (or incompletely explored) ends in the various pages of the notebook under that category.  

- Begin drafting the manuscript

Once the work has coalesced around a particular set of ideas and results appropriate for a single manuscript, I begin drafting the manuscript as a `.Rmd` file in the Github repository.   


----------------


The core of your case study is a diagrammatic workflow sketch, which depicts your the entire pipeline of your workflow. Think of this like a circuit diagram: boxes represent steps, tools, or other disjoint components, and arrows show how outputs of particular steps feed as inputs to others. This diagram will be complemented by a textual narrative.

We recommend the site draw.io for creating this diagram, or you are also welcome to sketch this by hand. While creating your diagram, be sure to consider:

* specialized tools and where they enter your workflow
* the "state" of the data at each stage
* collaborators
* version control repos
* products produced at various stages (graphics, summaries, etc)
* databases
* whitepapers
* customers

Each of the two example case studies include a workflow diagram you can also use for guidance.

Please save your diagram alongside this completed case study template.

##### Workflow narrative

Referring to your diagram, describe your workflow for this specific project, from soup to nuts. Imagine walking a friend or a colleague through the basic steps, paying particular attention to links between steps. Don't forget to include "messy parts", loops, aborted efforts, and failures.

It may be helpful to consider the following questions, where interesting, applicable, and non-obvious from context. For each part of your workflow:

* **Frequency:** How often does the step happen and how long does it take?
* **Who:** Which members of your team participate (or not)?
* **Manual/Automated:** Is the step automated or does it involve human intervention (if so, is it recorded)?
* **Tools:** Which software or online tools are used in the step? How are they used?

In addition to detailing the steps of the workflow, you may wish to consider the following questions about the workflow as a whole:

* **Data:** Is your raw data online?
   * Is it citeable?
   * Does the license allow external researchers to publish a replication/confirmation of your published work?
* **Software:** Is the software online?
   * Is there documentation?
   * Are there tests?
   * Are there example input files alongside the code?
* **Processing:** Is your data processing workflow online?
   * Are the scripts documented?
   * Would an external researcher know what order to run them in?
   * Would they know what parameters to use?

*(500-800 words)*

##### Pain points
*Describe in detail the steps of a reproducible workflow which you consider to be particularly painful. How do you handle these? How do you avoid them? (200-400 words)*

##### Key benefits
*Discuss one or several sections of your workflow that you feel makes your approach better than the "normal" non-reproducible workflow that others might use in your field. What does your workflow do better than the one used by your lesser-skilled colleagues and students, and why? What would you want them to learn from your example? (200-400 words)*

##### Key tools
*If applicable, provide a detailed description of a particular specialized tool that plays a key role in making your workflow reproducible, if you think that the tool might be of broader interest or relevance to a general audience. (200-400 words)*

##### General questions about reproducibility

*Please provide short answers (a few sentences each) to these general questions about reproducibility and scientific research. Rough ideas are appropriate here, as these will not be published with the case study. Please feel free to answer all or only some of these questions.*

1) Why do you think that reproducibility in your domain is important?

Reproducibility makes results more reliable, and more importantly, makes it easier to extend, test, and build upon existing results. 

2) How or where did you learn the reproducible practices described in your case study? Mentors, classes, workshops, etc.

Independent study of examples, experimentation, and reading, and connecting with other researchers sharing similar interests through the internet and social media. 

3) What do you see as the major pitfalls to doing reproducible research in your domain, and do you have any suggestions for working around these? Examples could include legal, logistical, human, or technical challenges.

Not a standard practice. In the short-term it takes more time.  It may also increase the probability of errors in your work being discovered.  

4) What do you view as the major incentives for doing reproducible research?

Making research easier to do.  Reproducible research facilitates collaboration, particularly with myself.  It improves my confidence in my own results and helps me build more efficiently on work that I have already done.  

5) Are there any broad reproducibility best practices that you'd recommend for researchers in your field?

Adopting tools that are widely used within my field (and others) for reproducibility.  These include: GitHub, Docker, rmarkdown

6) Would you recommend any specific websites, training courses, or books for learning more about reproducibility?

Any resources that teach these tools.  Additionally:

- The reproducible research workshop developed by NESCent: https://github.com/Reproducible-Science-Curriculum
