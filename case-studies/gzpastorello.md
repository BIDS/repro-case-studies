##### Introduction

1) Who are you and what is your research field? Include your name, affiliation, discipline, and the background or context of your overall research that is necessary specifically to introduce your specific case study.

My name is Gilberto Pastorello, and I'm a Computer Scientist at Lawrence Berkeley National Laboratory. My work involves developing data processing pipelines and data management solutions within the environmental domain.


2) Define what the term "reproducibility" means to you generally and/or in the particular context of your case study.

Within my case study, reproducibility means being able to apply standard methods to process heterogeneous data sets to generate comparable data products. The data sources for our processing are distributed across very different ecosystems, with data acquired, processed, and quality checked by different teams. The data products we generate from these data sets need to be in the same scales and have comparable levels of quality, representativity, etc.



##### Workflow diagram

[Diagram](gzpastorello.pdf)



##### Workflow narrative

Multiple Science Teams collect carbon, water, and energy fluxes from over 600 field sites across the world. These sites are operated independently and methods for data collection and processing by Science Teams can vary significantly, especially in terms of data quality levels. Our workflow aims at processing these heterogeneous data sets to generate data products that are comparable across these field sites. In this context, reproducibility is strongly related to identifying and documenting the right parameterization for processing, sequences of correction steps, and data filtering for each site.

The pipeline is executed every time new data are sent to us by Science Teams. We keep track of multiple submissions with a combination of simple incremental counters for versions and timestamps, and logs of data transfers (stored in a relational DB). Frequency of submissions range from daily to yearly updates from Science Teams. All executions of the pipeline generate a version, with successful executions being made public after being vetted by the Science Teams.

The first few steps are related to data quality and aim at identifying serious quality issues and making data quality more uniform across sites. Specialized algorithms and visualization methods are used in these steps, with automated generation of flags and manual inspection of data sets and metrics compiled on the data. Any decision to change the data sets is first shared with the Science Team and confirmed before being executed. Major issues are identified and solutions are developed in conjunction with Science Teams. An issue tracking system is used to keep track of interactions with Science Teams and issues being addressed in the data sets. The information in this issue tracking system is private, and accessible only to our team and the Science Team for the field site providing the data. Changes to data are also documented in log files and quality flags that are added to the data sets themselves, these being made available along with data products.  Changes and corrections generate a new version of the input data and the pipeline starts again with this new version.

The central portion of the workflow includes the processing steps for heat, carbon and micrometeorological variables. The parameters used to configure these executions are stored with the data sets, and specialized checks are also executed within each step. The results from most of these checks are stored as quality flags added to the data sets. Versions of the code used for these steps are also recorded in the data set's metadata.

The product merging step reformats the data into common structures and combines quality information into quality flags at a higher level of abstraction, to simplify using the data sets. The uncertainty quantification step generates quantiles representing confidence intervals for each of the processing steps. These quantiles are added as an uncertainty data product associated to each data processing step.

The product evaluation step involves looking at the resulting data products in the context of the field site by interpreting the results for physical feasibility and ecological significance. Checks against larger scale climate data are also performed. The information compiled in this step is shared with Science Teams for their evaluation and, if approved, shared publicly.

Data as shared by the Science Teams are made available online as well as the data products generated by our own processing. The code used for quality checks and product generation is going through an overhaul, with documentation and tests being added, and should be made available as a product in the future. Currently, the code is executed in a few institutions that can run the processing, with a shared private code repository holding the code. An external researcher would probably be able to run the code with default configurations, but likely would not be able to customize the execution for their own parameters - the code overhaul aims at allowing any user to be able to do just that.



##### Pain points

The execution of this pipeline can happen several times before acceptable results are reached, and it also includes several interactions with the Science Teams resulting in changes to the input data sets. The changes can be applied by the Science Teams or by our team. With multiple iterations, the changes that are needed to make a data set correct can be spread across many versions of the input data. Consolidating these changes is particularly difficult, especially when the changes were applied to versions that led to unsuccessful runs, which can be inadvertently ignored.

Keeping versions consistent across multiple processing instances can be a challenge. We use mapping of versions of inputs to outputs, but with outputs influencing what a new version of the inputs look like, these mappings are not always straightforward.

Many of the data sets we handle span decades of data collection, processing, and curation by multiple people. Since the teams are widely distributed and follow potentially different data collection and processing protocols, full reproducibility is nearly impossible to be achieved. A workaround has been to document the most likely choices in collection/processing in such cases.

Combining reproducibility issues with credit issues for data sets is another complicated issue. Giving proper credit to data providers and data processors/curators is an essential part in these large data sharing efforts. Data sharing policies are not always enough to allow proper credit to be assigned, especially when multiple versions of the data sets, spanning decades, are being shared.



##### Key benefits

Without the use of versioning and issue tracking, for instance, it would be impossible to fully document choices for the data sets that affect aspects such as data quality or data processing parameters. In this case, it would be nearly impossible for the project to succeed without these practices and tools.



##### Key tools

Data and software versioning are certainly the main practices adopted in this case study. We are currently working on assigning versions to data and software in a coordinated way, to allow assessment of simultaneous changes to data and code. Other tools adopted include a shared code versioning system and issue tracking systems for both the code and, more importantly, the data sets.



##### General questions about reproducibility

1) Why do you think that reproducibility in your domain is important?

The data sets in this case study offer unique insight into ecosystem-level behaviors for biological systems. An example of how these data are used is in the validation and assessment of climate models at much larger scales (regional, continental, and global). For this type of application, being able to recreate data products uniformly is essential for the credibility of these validations/assessments. This, in turn, directly affect credibility of climate models and the capacity of predicting future climatic trends.


2) How or where did you learn the reproducible practices described in your case study? Mentors, classes, workshops, etc.

Many of the practices were strongly motivated by requirements from the domain itself. Specific solutions were adapted and implemented following literature on handling these requirements -- e.g., versions, documentation for software parameterization, etc.


3) What do you see as the major pitfalls to doing reproducible research in your domain, and do you have any suggestions for working around these? Examples could include legal, logistical, human, or technical challenges.

Since a lot of the pipeline involves identifying data quality issues that can originate from human error, making documentation on these widely available is not an option -- the quality checks aim at improving quality and making data products more uniform and not at policing the work of data contributors.


4) What do you view as the major incentives for doing reproducible research?

Research involving large and/or distributed teams is much easier when allowing effective reproducibility. When multiple team members understand and can generate products, it is much more likely that problems will be identified early, questions from external members of the community will be answered more easily, and funding agencies will have documented product releases (data and software) to go along with publications for assessing scientific impact.


5) Are there any broad reproducibility best practices that you'd recommend for researchers in your field?



6) Would you recommend any specific websites, training courses, or books for learning more about reproducibility?

